---
title: "Deploying a Docker Container on Amazon EC2: A Step-by-Step Guide"
seoTitle: "Deploying a Docker Container on Amazon EC2"
seoDescription: "Learn how to package your application as a docker image and make It available for deployment!"
datePublished: Sun Jun 25 2023 09:24:48 GMT+0000 (Coordinated Universal Time)
cuid: cljb84kv0001309l1hws44e80
slug: deploying-a-docker-container-on-amazon-ec2-a-step-by-step-guide
cover: https://cdn.hashnode.com/res/hashnode/image/stock/unsplash/1cqIcrWFQBI/upload/c3e4c66325c360cfa651eb348b6df15f.jpeg
tags: docker, nginx, amazon-ec2, nextjs

---

In today's fast-paced world of software development, deploying applications quickly and efficiently is crucial. Docker containers have emerged as a popular solution for packaging applications and their dependencies, making deployment a breeze across different environments. When it comes to hosting Docker containers, Amazon EC2 provides a scalable and reliable infrastructure platform. In this step-by-step guide, we'll walk through the process of deploying a Docker container on Amazon EC2, allowing you to easily run your applications in a cloud environment.

We'll be using a Next.js 13 Todo App as an example application and Docker Hub as our container registry. Additionally, we'll explore the benefits of using Nginx as a reverse proxy to efficiently handle incoming requests. By the end of this guide, you'll have a solid understanding of how to package your app as a Docker Image, deploy your Docker container and configure Nginx as a reverse proxy.

### Prerequisites

* [Docker 101 Tutorial](https://www.docker.com/101-tutorial/)
    
* [Getting Started with Docker](https://www.docker.com/get-started/)
    
* [Docker CLI cheatsheet](https://docs.docker.com/get-started/docker_cheatsheet.pdf)
    
* An [AWS Account](https://portal.aws.amazon.com/billing/signup#/start/email)
    
* [Amazon EC2 Instance](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EC2_GetStarted.html)
    
* Read more about [NGINX](http://nginx.org/en/docs/beginners_guide.html)
    
* A web app to deploy or you are free to clone [my GitHub Repository](https://github.com/ankush-003/nexttodo)
    

### **Understanding Docker Images and Containers**

In the world of containerization, Docker has become a popular choice for packaging and deploying applications. To fully grasp the concepts and benefits of Docker, it's essential to understand the fundamental building blocks: Docker images and containers. Let's explore these concepts:

**Docker Images:**

* A Docker image is a lightweight, standalone, and executable software package that includes everything needed to run a piece of software, including the code, runtime environment, libraries, and dependencies. It is essentially a snapshot or template for creating Docker containers.
    
* Docker images are built from a set of instructions called a Dockerfile. The Dockerfile defines the configuration, dependencies, and steps required to create the image. Each instruction in the Dockerfile represents a layer in the image's filesystem, allowing for efficient storage and distribution.
    
* Docker images are typically stored in a registry, such as Docker Hub or a private registry. They can be shared, versioned, and easily pulled or pushed between different environments, making application deployment and distribution consistent and reproducible.
    

**Docker Containers:**

* A Docker container is a running instance of a Docker image. It is an isolated and lightweight runtime environment that encapsulates the application and its dependencies. Containers provide consistency and portability, allowing applications to run reliably across different systems, from development to production.
    
* Containers are created from Docker images using the `docker run` command. Each container is an isolated process that runs on the host machine but is independent of other containers, providing process-level isolation and resource management.
    
* Containers have their own filesystem, network, and process space, allowing multiple containers to run concurrently without interfering with each other. They are ephemeral, meaning they can be started, stopped, deleted, and replaced easily, promoting scalability, resilience, and agility.
    

**Benefits of Docker Images and Containers:**

* **Portability**: Docker images and containers provide a consistent runtime environment, enabling applications to run on different systems without worrying about dependencies and configuration discrepancies.
    
* **Isolation**: Containers offer process-level isolation, ensuring that applications and their dependencies are encapsulated and do not interfere with each other. This isolation improves security, stability, and scalability.
    
* **Reproducibility**: Docker images are built from declarative Dockerfiles, making the build process reproducible and version-controlled. This allows developers to recreate the exact environment needed for their applications.
    
* **Scalability**: Docker containers can be easily scaled horizontally by running multiple instances of the same container image. This simplifies load balancing and allows applications to handle increased traffic efficiently.
    
* **Efficiency**: Docker images leverage layer-based storage and a caching mechanism, enabling efficient storage and transmission of images. Containers are lightweight and start quickly, optimizing resource utilization.
    

Understanding Docker images and containers is crucial for harnessing the power of Docker and achieving efficient application deployment, scalability, and reproducibility. With Docker, you can package your applications and their dependencies into portable units, ensuring consistency and reliability across different environments.

### Building & Pushing Docker Image

First, we'll build the docker image and push it to DockerHub, this will allow us to easily retrieve the image on our EC2 instance later. Here is the Dockerfile that we'll be using to build the Docker image.

```dockerfile
FROM node:lts-alpine
WORKDIR /app
RUN npm install -g pm2
COPY package*.json ./
RUN npm install
COPY . .
COPY prisma ./prisma
RUN npx prisma generate
RUN npm run build
EXPOSE 3000
ENV DATABASE_URL = <your database url>
CMD ["pm2-runtime", "start", "npm", "--", "start"]
```

The provided Dockerfile is used to build a Docker image for a Node.js application using [PM2](https://pm2.keymetrics.io/) as the process manager. Let's break down the different sections:

**FROM node:lts-alpine**

* This line sets the base image for the Dockerfile, using the official Node.js LTS (Long Term Support) version based on Alpine Linux. Alpine Linux is a lightweight distribution, perfect for containerized environments.
    

**WORKDIR /app**

* Specifies the working directory inside the container where the application code will be copied and executed. In this case, it is set to `/app`.
    

**RUN npm install -g pm2**

* Installs PM2 globally inside the Docker container using npm. PM2 is a popular process manager for Node.js applications, providing advanced features for process management, monitoring, and deployment.
    

*COPY package.json ./*\*

* Copies the `package.json` and `package-lock.json` (if present) files from the host machine to the `/app` directory inside the container. This step allows Docker to cache the dependencies installation process separately, optimizing build time.
    

**RUN npm install**

* Executes `npm install` inside the container, installing the application dependencies based on the copied `package.json` file. This step will make use of Docker's cache if the `package.json` file hasn't changed since the previous build, saving time during subsequent builds.
    

**COPY . .**

* Copies all the application source code from the host machine to the `/app` directory inside the container.
    

**COPY prisma ./prisma**

* Copies the `prisma` directory from the host machine to the `/app/prisma` directory inside the container. This step is specific to the application's needs.
    

**RUN npx** [**prisma**](https://www.prisma.io/) **generate**

* Executes the Prisma CLI command `npx prisma generate` inside the container. Prisma is a database toolkit used to generate database models and interact with databases.
    

**RUN npm run build**

* Runs the `npm run build` command inside the container. This command is typically used to build the application code, preparing it for production deployment.
    

**EXPOSE 3000**

* Exposes port 3000 from the container. This indicates that the application inside the container will listen for incoming connections on port 3000.
    

**ENV DATABASE\_URL = &lt;your database url&gt;**

* Sets an environment variable `DATABASE_URL` with the specified value. This is specific to the application's needs and should be replaced with the actual database URL.
    

**CMD \["pm2-runtime", "start", "npm", "--", "start"\]**

* Defines the command that will be executed when the container starts. In this case, it uses PM2 runtime to start the Node.js application using the command `npm start`.
    

The Dockerfile configuration optimizes build times by utilizing Docker's cache mechanism, ensuring that dependencies are only installed if the `package.json` file has changed. Additionally, PM2 is used as the process manager to manage the application's execution and ensure its availability.

Now let's build the docker image and push it to DockerHub.

```bash
# build docker image from Dockerfile
docker build -t <image-name> .
# running docker image
docker run -dp 3000:3000 <image-name>
# pushing docker image to your DockerHub
docker tag <image-name> <Dockerhub-username>/<image-name>
docker push <Dockerhub-username>/<image-name>
```

After executing the command `docker run -dp 3000:3000 <image-name>`, the Docker container of your application is now running on port 3000 of your localhost. This means that you can access your application by opening a web browser and navigating to [`http://localhost:3000`](http://localhost:3000).

By mapping the container's port 3000 to the host's port 3000 using the `-p` flag in the `docker run` command, you have established a connection between the running container and your local machine. This allows you to interact with the application as if it were running directly on your local host.

To test the successful deployment, simply open your preferred web browser and enter [`http://localhost:3000`](http://localhost:3000) in the address bar. If everything is set up correctly, you should be able to see and interact with your application in the browser window.

This mapping of ports enables seamless communication between your local machine and the Docker container, making it convenient for testing and accessing your application during the development and deployment process.

[![Application running on port 3000](https://cdn.hashnode.com/res/hashnode/image/upload/v1687554927740/937ea8f8-df1d-4e62-bf48-e964e2c93443.png align="center")](https://nexttodo-rho.vercel.app/)

[Here](https://hub.docker.com/repository/docker/ankush003/nexttodo/general) is the DockerHub repository for the docker image of this application. Docker Hub is a cloud-based repository provided by Docker that simplifies the storage, distribution, and collaboration of Docker images.

[![](https://cdn.hashnode.com/res/hashnode/image/upload/v1687681544817/a3e69188-f435-49d4-9a19-b4b88007fe30.png align="center")](https://hub.docker.com/repository/docker/ankush003/nexttodo/general)

### Setting up an Amazon EC2 Instance

Amazon Elastic Compute Cloud (EC2) is a web service provided by Amazon Web Services (AWS) that allows you to launch virtual servers, known as instances, in the cloud. Amazon EC2 instances offer a scalable and flexible infrastructure for running your applications in the cloud. With a wide selection of instance types, operating systems, and integration with various AWS services, EC2 empowers businesses to deploy and manage their workloads efficiently and cost-effectively.

Here are the configurations for the Amazon EC2 instance that we'll be creating using Ubuntu, you can also refer to [Amazon docs](https://docs.aws.amazon.com/efs/latest/ug/gs-step-one-create-ec2-resources.html) to understand the steps properly.

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1687640440372/b8386d7f-9c46-4b88-835a-5b180efc6eff.png align="center")

We can connect to the instance using [SSH](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AccessingInstancesLinux.html) through the following commands \[optional\]

```bash
chmod 400 <path to private key file>
ssh -i <path to private key file> <public DNS of your instance>
# to run as admin after SSH is successful [optional]
sudo su
```

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1687632364775/8f23fceb-1b6c-4dc0-844d-42ed45f4b8a2.png align="center")

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1687680597487/996cab54-d944-4c8d-bc5d-92e6996cbc66.png align="center")

Add HTTP and HTTPS rules and IP anywhere to the [Security Group](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/working-with-security-groups.html) of our instance.

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1687675964955/f9d9b864-ebf9-4ae0-8aba-ceb57528848c.png align="center")

### Installing Docker on our Amazon EC2 Instance

run the following commands to install Docker on our Amazon EC2 Instance

```bash
sudo yum install docker
sudo systemctl start docker
```

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1687680498336/44218b61-f7d3-45b6-bc38-668b18f745d0.png align="center")

Run the following commands to download and run the Docker image that we pushed previously to DockerHub.

```bash
sudo docker pull ankush003/nexttodo
sudo docker run -d ankush003/nexttodo
# to view all the running containers
sudo docker ps
```

The command `sudo docker run -d ankush003/nexttodo` is used to run a Docker container based on the `ankush003/nexttodo` image in a detached mode (in the background).

### Installing Nginx on our Amazon EC2 Instance

**Nginx: High-Performance Web Server and Reverse Proxy**

Nginx is a powerful open-source web server and reverse proxy server known for its high performance, efficiency, and scalability. Here's a brief overview of Nginx:

* **Web Server**: Nginx can function as a standalone web server, serving static content and handling dynamic requests. It efficiently delivers web pages, images, and other static files to clients, providing fast and reliable performance.
    
* **Reverse Proxy**: Nginx is widely used as a reverse proxy server, sitting between clients and backend servers. It receives client requests, forwards them to the appropriate backend server, and returns the response to the client. This helps distribute traffic, improve security, and provide load balancing and high availability for backend services.
    
* **Performance and Efficiency**: Nginx is renowned for its exceptional performance and low memory footprint. It is designed to handle a large number of concurrent connections with minimal resource consumption, making it suitable for high-traffic websites and applications.
    
* **Caching and Content Delivery**: Nginx includes built-in caching capabilities that allow it to cache responses from backend servers. By serving cached content directly to clients, Nginx reduces the load on backend servers, improves response times, and enhances overall performance. It can also be used as a content delivery network (CDN) edge server to deliver cached content closer to end users.
    

Nginx's exceptional performance, versatility, and extensive feature set have made it a popular choice for web servers, reverse proxies, and load balancers. Its efficient architecture and flexible configuration options make it suitable for a wide range of use cases, from serving static content to managing high-traffic websites and complex application architectures.

Run the following commands to install nginx on our Amazon EC2 Instance

```bash
sudo yum install nginx
sudo service nginx status
sudo service nginx restart
```

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1687680413935/41f3bca8-1be2-4fe9-9a9b-5afe87eb4031.png align="center")

### Configuring NGINX

Run the following commands to open the `nginx.conf` file that we'll be editing to configure NGINX as a [reverse proxy](https://www.nginx.com/resources/glossary/reverse-proxy-server/).

A proxy server is a go‑between or intermediary server that forwards requests for content from multiple clients to different servers across the Internet. A **reverse proxy server** is a type of proxy server that typically sits behind the firewall in a private network and directs client requests to the appropriate backend server. A reverse proxy provides an additional level of abstraction and control to ensure the smooth flow of network traffic between clients and servers.

```bash
sudo cd /etc/nignx/
sudo nano nginx.conf
```

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1687680236832/93878f85-2439-4e4a-99bb-fe8192e39ba7.png align="center")

Add the following code snippet to the server block in the `nginx.conf` file.

```bash
location / { 
        proxy_pass http://172.17.0.2:3000;
}
```

In a Docker environment, each container is typically assigned an IP address within a virtual network. The IP address we provided, `172.17.0.2`, is a commonly used default IP address for the first container created on the Docker network. However, it's important to note that the specific IP address assigned to a container may vary depending on the Docker network configuration.

When using Nginx as a reverse proxy in a Docker environment, we need to ensure that the `proxy_pass` directive points to the correct IP address and port where our backend container is running. Here's how you can modify the `proxy_pass` directive to work with Docker's default network:

```bash
proxy_pass http://<container_ip>:3000;
```

Replace `<container_ip>` with the actual IP address of your backend container running the application. To obtain the IP address, you can use the following command:

```bash
docker inspect <container_name_or_id> | grep -i "ipaddress"
```

Replace `<container_name_or_id>` with the name or ID of your backend container. This command will output the IP address assigned to the container.

1. **Reverse Proxy Configuration**: The added configuration block `location / { ... }` is configuring Nginx as a reverse proxy. A reverse proxy acts as an intermediary server between client requests and backend servers. By proxying requests, Nginx can handle incoming requests, forward them to the backend server, and return the response to the client.
    
2. **Load Balancing and Scaling**: Using a reverse proxy like Nginx allows you to implement load balancing and scaling strategies. By configuring Nginx to proxy requests to multiple backend servers (identified by their IP addresses and ports), you can distribute the incoming traffic across these servers. This helps distribute the load, improve performance, and ensure the high availability of your application.
    
3. **Separation of Concerns**: Separating the responsibilities of serving static files and handling dynamic requests can improve the overall performance of your application. Nginx is known for its efficient handling of static content, so by letting Nginx serve static files directly, you offload that responsibility from your Node.js application, allowing it to focus on processing dynamic requests.
    
4. **Security and Performance Enhancements**: Nginx provides various features to enhance the security and performance of your application. It can handle SSL/TLS termination, caching, rate limiting, and more. By incorporating Nginx into your application's architecture, you can leverage these features to improve security, reduce latency, and optimize response times.
    

You can read more from [NGINX docs](https://www.nginx.com/blog/setting-up-nginx/#proxy-server).

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1687680300221/f1aabbe0-b2af-4201-9a9f-600651aadd64.png align="center")

To reload the `nginx.conf` configuration file and apply any changes made, you can restart the Nginx service. Use the following command to restart Nginx:

```bash
sudo service nginx restart
```

Executing this command will gracefully restart the Nginx service, ensuring a smooth transition and minimal downtime. Upon restart, Nginx will read the updated `nginx.conf` file, incorporating any modifications or additions made to the configuration.

Reloading Nginx is necessary whenever you make changes to the `nginx.conf` file, such as adding or modifying server blocks, configuring proxy settings, or updating SSL certificates.

### Deployment Successful!

You can view your deployment by heading over to the public DNS of your Amazon EC2 Instance.

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1687679801397/c839cb72-7d55-49e3-b795-3ef5dc9740e8.png align="center")

***Congratulations on successfully deploying your application using Docker and Amazon EC2!***